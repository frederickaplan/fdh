---
title: 'Foundations in Digital Humanities 1.3'
subtitle: 'Big Data of the Past'
author:
 - Frederic Kaplan

# Don't change the following lines
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyfoot[L]{-release-version-}
output: pdf_document

---

# FDH-1-3: Big Data of the Past

#### Core theses explored in this chapter

- There are plenty of precious structured data in the past.
- These datasets are complex to extract due to the evolution of regulated representations and the recursive nature of redocumentation processes.

#### Contents:
> 1. Introduction : Geneva 1896
> 2. Planterary-scale interfaces
> 3. The Information Mushroom
> 4. Data Acceleration Regimes
>> 4.1. Data acceleration regimes in History // 4.2. The reconstruction of knowledge systems //
> 5. Regulated Representations
>> 5.1. Defining regulated representations // 5.2. Focus: The regulation of cartography // 5.3. Regulated representations: From tool to machine // 5.4. Focus: Simondon and concretization processes // 5.5. Studying the transformation of regulated representations through time //
> 6. Inferred Patterns: From data to models
>> 6.1. Three data dominions // 6.2. From predictive to prescriptive systems //
> 7. Redocumentation processes
>> 7.1. Recollecting and remapping // 7.2. New formalisms and the reshaping of data // 7.3. The solution to remapping artefacts: Fixed points // 7.4. Filling the gaps: Recursive redocumentation
> 8. Fictional spaces
>> 8.1. Trusting historical information // 8.2. Disjointed fictional spaces // 8.3. Joining fictional spaces
> 9. Conclusion
>>9.1. Summary // 9.2. In the next chapter //

## 1. Introduction : Geneva 1896

In 1896, in Geneva, the two million visitors to the national exhibition could for the first time discover their city from the air. In small groups, from the gondola of a helium balloon, they could see how the city had changed considerably in recent decades. Following the footsteps of other European capitals at the end of the 19th century, the old medieval city had now become an open, airy, cosmopolitan city, built for walking rather than defence, a city of its time. This view of the sky, absolutely unheard of at a time when air travel was still only a dream of the future, made it possible at a glance to embrace the structure and motifs of the new city. It offered a viewpoint and an understanding that no speech could have summed up.

Let us try to imagine the experience of those visitors at the end of the 19th century, who were not accustomed to aerial views and who for many were boarding a balloon for the first time. The exhilaration of the panorama comes from a sudden transformation of the relationship between space and time, a _spatiotemporal compression_. From a single point, it suddenly becomes possible to glimpse in a new form the places that took hours to reach. The sudden overabundance of available information makes you dizzy. Where to look? The eye is desperately looking for global motives or on the contrary for a singular detail to make this spectacle too rich, too dense, to make it intelligible, to make it return to the order of the discourse, to be able once back down to say what we have seen. But the panorama only allows us to tell the story in a very partial way. Once down, visitors will generally have to be content to talk about the vertiginous experience as such rather than its content.

Still at the Geneva World Fair, but on land this time, as an echo to this first vertiginous panorama, another type of show was proposed in a pavilion in the Parc de Plaisance. This was a large model of almost 30 m2, representing Geneva in 1850 and entitled "Grand Relief du Vieux Genève". Despite the large dimensions of this three-dimensional model, the old city was reconstructed with great care. Visitors looking down over the model were once again placed in a panoramic view, but this time facing the artificial reconstruction of a vanished landscape.

The author of the model, Auguste Magnin (1841-1903), had spent almost 18 years meticulously building this model in the attic of his architect's studio. The whole thing weighed almost 670 kilos. For this project, he had first gathered extensive documentation, drawn up field surveys, traced maps and views of the town and finally drawn up final plans for each group of houses. Magnin had made initial cardboard models to assess the complexity of the task and dimension his project, and then gradually converged on a combination of processes to build the model. It was based on a wooden structure supporting an assembly of 120 caissons, followed by a model of the terrain using one-centimetre thick planks. Facades and walls were built from welded zinc sheets. Roof tiles and street paving stones were imitated by electroplating. By means of acid etching, he was able to reproduce the paving of the walls of the fortifications. The 1500 trees in the model were made of cast iron, all different. Variations in the arrangement of the branches suggested the shape of a plane tree, a poplar or a fir tree. Magnin had chosen to keep the zinc and copper in their natural colour, favouring above all the aesthetics and overall rendering of the model rather than pursuing an impossible attempt at realism.

Using archives, land surveys and old maps, Magnin had to reconstruct the underlying structure of the Geneva of 1850, understand the architectural and urban grammar, and then choose the technical processes to recreate what finally became a simulation of the vanished city. At each stage he had made multiple choices. To increase the legibility of the recreated model, Magnin had not hesitated to work on three different scales: 1/250th for the plan, 1/200th for the elevation and 1/100th for the terrain. If Magnin had kept a constant scale, the city would have been crushed. By distorting the heights, Magnin's model gained in visual efficiency, and better reflected the details of the city, even if on principle it lost its fidelity and ceased to be an absolutely realistic model of the ancient city.

Magnin was precisely motivated to work on this long-winded work by the great urbanistic upheavals that were affecting Geneva in the second half of the 19th century. Geneva was indeed given a new structure. A new urban grammar was to cover the Old City. Magnin chose the symbolic date of 1850, as it corresponded to the first blows of the pickaxe to the fortifications of the Huguenot city, the beginning of the end of a world. For him, it was not so much a question of recording its traces as of making it visible again, in the most impressive and educational way, using the technology of his time. The panorama of old Geneva was to be as spectacular as that of the New City seen from the sky. The work of a single man who was struggling to bring the past to life in a changing world, the Magnin relief carries within it the ambiguities of the work of fine granular historical reconstruction, a question which is, as we shall see, eminently contemporary. The Magnin relief was not a reproduction, but an informed reinvention of the past, an attempt to make the past present.

## 2. Planterary-scale interfaces

In less than ten years our relationship to space has changed considerably. With just a few clicks we can now from a distant view of the Earth, quite similar to the first images taken by the Apollo astronauts, zoom in and see what a region looks like as if we were a bird flying over it. Often we can also see what a building looks like from a nearby street and literally move around as if we were there. It took only a few years for the planetary urban space to become an algorithmic space, mapped, photographed, articulated to be explorable through the interfaces of our computers. The globe was not only an image, it was transformed into a machine.

The profound consequences of these fantastic vision devices are still to be studied. But one remark is immediately obvious: **time seems singularly invisible to this device**.  Is it really the present that the algorithmic globe gives me to see? How old are the aerial images shown to me by the interface? Were they taken at the same time as those used to build the immersive navigation that now allows me to see the same building in profile?  I look at this image taken of a city as the crow flies, then I dive into one of these streets without knowing if this new shot was captured the same year as the aerial view. In fact, when I think about it, I know that these two ways of documenting and representing space are not synchronised. I have no idea when the images were taken by the satellite or the aeroplane that allow us to see the city from above, or when the car that collects the images of the facades of the buildings passed through these streets. We go from one vision device to another, in complete continuity, as if we were living in a perpetual present utopia.

From this blurring of boundaries emerges the illusive idea that we can perceive the world as it is **_now_**, so much that debates and hypotheses about the future technological breaks seem to have disappeared from the collective discourse, to be **replaced by a continuum of minor innovations**.

This travesty of time seems to be a characteristic feature of the information systems that have developed in the first decade of the 21st century. The information deluge that we have been experiencing for several years now has led each actor to face up to the most pressing need: to organise the world's information, to categorise it, to articulate it to form large cartographic systems and large databases capable of bringing order to chaos. In addition to the possibility of tracing in real time the movement of people and goods, the exchange of messages, financial transactions, the development of generalised audio-visual capture systems from video surveillance to satellite imagery, has been added the opportunity for each owner of a portable computer device to document his or her own life in a way that was never possible before. Each of this captured information is potentially indexed in time and space, integrating into a four-dimensional coordinate system, but always according to a logic of stability of our modes of representation. However, nothing is less stable over time than the modes of information organisation.  

## 3. The Information Mushroom

If we were indeed faced with "Big data from the past", could we imagine extending the most popular services on the Internet to give them back what they sorely lack: duration, long time? Could we imagine a "Google map of the past" equipped with a "slider" allowing us to see the same place 50, 100 or 1000 years earlier? Could we imagine reconstructing a "Facebook of the past", reproducing the links that united millions of people in the Middle Ages, chronicling their lives with a density equivalent to that which characterises our life stories today? Perhaps building a time machine today would mean exactly that: making the past as present as the present, integrating it into the global information system.

One way of thinking about this perhaps impossible dream is to think of the amount of digital information we have about each era as a **mushroom**. If we place time vertically and the amount of information available horizontally, the information deluge of the last ten years can be visualised as a large horizontal plateau resting on a base that keeps getting smaller and smaller as we go down into the past. To build a Google map or a Facebook of the past, we need to somehow transform this mushroom into a rectangle, widen its base to make it comparable to the size of the table, somehow obtain an information density that is as important for the past as it is for the present.

A first approach consists in conducting vast digitisation projects, then systematically extracting information from these digitised collections.
- The **Google books project** (cf. _Lecture 2-2_ for more details) has thus digitised almost 30 million books and transcribed with automatic reading software a large proportion of them, making them indexable and searchable. This database, even if it is made up of extremely disparate elements (what could be more different from one book than another) is nevertheless a very rich source of information.
- The major projects to **digitise the press** are moving in the same direction, making it possible to obtain detailed information on local and international events on a daily basis, but also stock market prices, train timetables, classified ads, etc.
- The digitisation of **administrative data** provides systematically structured information documenting births, deaths, marriages, wills, changes of ownership or cadastral changes. An extremely large number of private archives have been added to this already very rich collection of information with photographs and letters.

The archival and documentary logic put in place at the beginning of the 19th century in France or Italy, for example, still has much in common with that which is at work today. For this reason, there is no doubt that over the last two hundred years, for example, by increasing the information extracted from press articles and administrative records, a relatively accurate picture of a country's society and its day-to-day development can be reconstructed. Obviously, the further back we go in the past, the more the number of documents is reduced and, above all, **the more the logic of representation becomes alien**. The documentary structures put in place during the Renaissance, in line with the spread of printing, require specific work to be interpreted according to the logics of contemporary information systems. The documentary situation in the Middle Ages and earlier periods obviously poses many other challenges.

To compensate for the lack of archival information or to complete the spaces not covered by the archives, we can adopt a complementary strategy that is not foreign to the historian by reconstructing the missing data by **extrapolation and generalisation**. If a historian finds the logbook of a 16th century ship's captain documenting precisely a voyage between Venice and Corfu, he will not only be able to draw conclusions about that particular voyage but will, under certain conditions, be able to use this document to deduce information about the modes of navigation and maritime life of that period. In the same way, we can use architectural information from a Venetian palazzo in Gothic style which is still very well preserved to make hypotheses about the structure and style of a building of the same period and with similar functions but which is now completely destroyed.

It is always a question of going beyond the information contained in a particular document, extracting structures, grammars and using them to build motivated hypotheses that allows us to fill in the blanks left between the archives. In computer science, these techniques correspond to the large family of simulation methods that have been studied for more than 50 years.  The problem of discovering structures in noisy data, of generalisation from examples, of extrapolation according to certain working hypotheses is the basis of a science which has developed in parallel with the historical sciences.  The "Big data of the past" may herald a fertile and new interface between these fields and in any case invite reflection on the singular epistemological status of these "reconstructed pasts".

Simulation cannot be understood as a way of simply compensating for the lack of historical data, used only when archival data is lacking. In fact **there is never enough data**. No cadastral plan, no photo, no laser survey, could allow me to reconstruct precisely the structure of a single _calle_ in Venice. Even in situations of "hyper-documentation", at some point the data has to be extended according to certain hypotheses. Simulations and data always go hand in hand. Only the resolution and the uncertainty change.

As a metaphor, the **Information Mushroom reveals the hidden part of the Iceberg**. There is still a considerable amount of information about the past to be dug and, through that, a significant power to be accessed (_"Scientia potentia est"_). However, the very nature of information in documents from the past is, of course, much more complex and needs further examination.

## 4. Data Acceleration Regimes

Big Data is not a new phenomenon. History is punctuated by several Big Data moments which are characterized by a **widespread, shared sense of information overload alongside rapid societal acceleration accompanied by the invention of new intellectual technologies**.

It is in those "Big Data moments" that the four dimensions of Data Bigness (cf. FDH-1-2) show the potential of their intrinsic relationships:

Technology (e.g., more computing power, increase in communication speed) enables Big Data by making it possible to produce open-ended streams of data. This new data stream, operating in a newly standardised environment, facilitates the creation of new relations with one another. Out of the opportunities created from these network sets of new data emerges a new relationship to knowledge, leading in some cases to a paradigm shift. **This self-reinforcing loop creates what can be called, a data acceleration regime.**

### 4.1. Data acceleration regimes in History

Note that **data acceleration regimes are not unique to contemporary massive datafication**; rather, they echo other moments in history. For example, in Mesopotamia, a large empire developed standardized administrative rules to cope with the new complexity of good circulation and population management.

The invention of a new writing and accounting technology enabled the standardization of data streams materially embedded in clay tablets. The resulting information systems gave birth to an early science of planification that surely played a key role in the long-lasting power of these empires.


Generally speaking, in order to be translated into relevant sources of information about our past, datasets produced in these moments of acceleration **need to be remodelled and reinterpreted**.

#### 4.1.1. Focus: Ebla's Tablet

- In contemporary Syria, 60 km south of Aleppo, in the now totally destroyed ancient city of Ebla, 17,000 argyle tablets and fragments were discovered.
- The tablets, written in Sumerian and Eblaite, constitute an antique administrative archive documenting with precision the life of this city, which was one of the most powerful of this region between 2500 and 2400 BC.
- This ancient information system provides a valuable example of how massive information about economic, diplomatic, and commercial exchanges were recorded and used several millennia ago.

#### 4.1.2. Focus: Annales Maximi

- In Rome, the Annales Maximi – 80 books from 400 BC to 130 BC – were the products of a very different early recording machine for capturing streams of events.
- The Pontifex Maximus, chief priest of the Capitoline, systematically maintained a detailed record of key public events, including the names of the involved magistrates and other important events such as famines, battles, extraordinary phenomena, and treaties
- Contrary to the lightweight and easily erasable argyle tablets—facilitating information management, accumulation and control—the Annales engraved, locally and in a stable manner, information that could resist centuries of war.

Likewise, The Roman Empire’s need to unify the circulation of goods, person, and information, and to exercise societal and military coordination over an extremely vast territory gave birth to additional forms of writing technologies and record-event handling methods.

Standardized information started to spread all over the Empire, from the Mediterranean region to the territories now comprising Great Britain and Germany. The resulting paradigm shifts in terms of global governance marked a watershed in information management.

#### 4.1.3. A response to an initial societal stress

Data acceleration regimes generally start with an initial societal stress. This can, for instance, occur when a governing entity needs to cope with an unexpected, intrinsic, and complex evolution of its territory or social structures, or in the case of unexpected encounters with new populations and cultures. **Covid-19 is, of course, a recent example of a data acceleration regime due to a societal stress.** 

In response to this stress new information processes are created. The acceleration in itself starts when a new technology enables massive data production that follows regular patterns by defining specific production constraints (e.g., administrative rules, printing industries, scientific experiments).

#### 4.1.4. Focus: The Renaissance acceleration

For Europe, one classically discussed case of “acceleration” is the Renaissance, linked with the advent of the printing industry (see chapter 2-3), the discovery of Asia and the Americas, and the globalization of exchanges all over the world. Not only were new editions of ancient texts starting to be printed and circulated but also a deluge of “how-to books” explaining previously secret arts and methods.

This **sudden increase in knowledge** and exposure to new practices created a well-documented feeling of information overload: there was definitely “too much to know”. Likewise, the discovery of new species in Asia and in the Americas challenged the capacity of scholars to recognize and classify natural beings.

Organizing the steady stream of new species was an extremely demanding endeavor that called for **new intellectual paradigms**.

Eventually, the globalization of monetary exchanges and the increased complexity of trade networks challenged the traditional methods for tracking commercial processes and advanced the rise of more mathematically sound, standardized methods.

From a technological perspective, the Renaissance and early modern period were intrinsically linked with the **invention of several intellectual technologies** for search and retrieval: indexes, bibliographies, accounting tables, and hierarchical collection structuring methods in addition to chronologies and maps.

From an open-ended perspective, the acceleration of exchanges, the rise of the printing press industry, and the early attempts to conduct experimental science contributed to producing streams of new data.

From a relational perspective, both early modern collections, which attempted to create a system for organizing natural and artificial entities and the double accounting system, which enabled a new tractability of economic exchanges at the global level helped advance the fundamentally network nature of the new datasets.

Eventually, from the paradigmatic perspective, the early modern **_episteme_** reframed entire views of the world both past and present into new coherent systems of knowledge, introducing, for instance, tree-based genealogical approaches in early natural history or philology.

### 4.2. The reconstruction of knowledge systems

Following the Big Data multifaceted criteria we previously introduced, the Renaissance and early modern periods qualify as data acceleration regimes, even if the size of the datasets managed seem small compared to contemporary standards.

These epochs have produced datasets structured using the specific intellectual technologies and following the epistemic paradigms of their time.

These datasets, if interpreted correctly, could be precious for **reconstructing entire systems of knowledge**.

From the antique administrative structures to the new information logics (cadaster, census) that accompanied the industrial revolution in the nineteenth century, how many data acceleration regimes can be identified? Can we clearly segment them? Are they more easily identifiable by their enabling technology, by the volume of data they produced, by the new connections they enabled, or by the intellectual shift they introduced?

Developing methods for mapping data acceleration regimes in space and time is a crucial challenge for reconstructing Big Data of the past. This global data census could take the form of a digital historical atlas, thereby reconstructing — from a distance — great as well as minor moments in the world’s information history.

## 5. Regulated Representations

### 5.1. Defining regulated representations

The common trait of all these ancient recording technologies — beyond their differing physical materialities — is their capacity to deal with an open-ended stream of information and reorganize it to fit a given information paradigm, creating new relations between them. We can call them **_regulated representations_**.

A representation is a man-made material document that stands for something else, typically a complex, highly dimensional event or phenomenon. For instance, a photographic picture of a scene, a sculpture of a Greek hero, a theatrical play, or a novel is man-made representations.

A regulated representation is a particular case of representation **governed by a set of production and usage rules**. These rules can be intrinsically embedded in the production process of the representation or the result of cultural conventions.

Examples of regulated representation include indexes of names, accounting tables, family trees, flow-chart diagrams, formal processes, and maps of a region.

On the contrary, the production of a sculpture, a painting, or a theatrical play is generally too weakly regulated by conventional rules to be considered an example of a regulated representation.

### 5.2. Focus: The regulation of cartography

There are obviously different qualitative levels of regulation rules. Maps are good examples of **how regulation and production rules progressively structure themselves over time**.

Modern conventions when creating a map, such as the indication of scale and the direction of North, were progressively introduced over time; the associated reading skills (how to handle a map, how to interpret its convention) developed in parallel.

The following four examples of maps show the emergence of regulating rules in cartography:

- **Gough Map**, c. 1360 – In this medieval map of England, the East direction is drawn up (a common trait of pre-Renaissance maps) as the orientation is not yet following the North-up convention. It nonetheless breaks with past maps and the theology-minded cartography by attempting to faithfully represent the geography of the Great-Britain. The coasts are relatively accurately drawn. It is also the first known map to show the road network in England. It, however, visibly suffers from a lack of knowledge in some regions (most notably Scotland). On the other hand, the southeastern region is quite detailed if not accurate, prompting some scholars to conclude that the map drawer(s) likely came from the South-East (which is in agreement with the linguistic analysis of the map's text).
- **Piri Reis Map**, 1520 – This map of european coastlines has clearly been drawn from the point of view of the sea, with coasts being the only details drawn. Production rules have been tightened, resulting in a much more accurate map.
- **Earth Elevation Map**, 2006 – This map has been drawn using sensor data. The process is automated, and does not rely on humans anymore. The production rules are built _within_ the machine. _Conventions have evolved into a mechanism._
- **Google Maps**, 2004-... – The final step in this regulation mechanism is seens with the example of Google maps, which in a perpetuous state of improving. Most of the features are not printable, because they have been designed to accessed on a machine.

### 5.3. Regulated representations: From tools to machines

The general process of this regulating tendency involves the transformation of conventions into mechanisms. The regulation usually proceeds in two consecutive steps, mechanizing first the representation’s production rules and then its conventional usages. Ultimately, through this process, regulated representations tend to become machine readable.

In the case of maps, the mechanization process is begun by a progressive formalization of the recording, gathering, storage, and unification of geographic information. This corresponds to the mechanization of conventional production rules. Paper maps were still produced, sharing similarities with those of the previous generation, but they were made in a completely different manner.

The next stage was the **mechanization of usage conventions**, transforming the regulated representation into machines in which all the possible usages are explicitly treated. The digital maps we use nowadays permit a large set of operations (scaling, rotation, etc.) and offer ways to handle multiple information layers. As machines, they offer many more possibilities than traditional ones. However, these various new modes of usage are explicitly programmed.

A paper map can be used freely for purposes other than its original function. In this sense, **it is still a tool**. A digital map can only be accessed through specific input and output commands; it has internalized its own usage rules. **It is therefore a machine**.

### 5.4. Focus: Simondon and concretization processes

The mechanization process may not immediately produce changes in usage, but the changing nature of the representation results in technological synergies and aggregation effects. As maps became machines, they progressively merged into a global mechanic system in which a multitude of maps became aggregated into a single one.

As regulated representations become more regular, they tend to aggregate into unified systems.


These dynamics have been well described by Gilbert Simondon as **_concretization processes_**:
- Different systems put in contact for enough time, they eventually merge to form a new "concretized" system.
- Cars were initially nothing but the aggregation of various mechanical parts (engine, wheels, brakes) before becoming, through the passing of time, a single new "machine". But this is true for any agglomerated system: the more the production and usage rules are internalised in new machines and mechanisms, the more the system eventually becomes unified and, perhaps more importantly, indivisible.

### 5.5. Studying the transformation of regulated representations through time

Given the unification tendency of regulated representations, their evolution can be likened to a converging tree system in which various branches progressively merged to create larger standardised sets of production and usage rules.

The great challenge at this stage is to come up with formalisms capable of modelling different families of regulated representations, and therefore to consider their evolution and transformation through time.

## 6. Inferred Patterns: From data to models

**As soon as regulated representations and structured data are collected, it becomes possible to infer patterns**. In most cases, gathering data about the present is motivated by the desire to produce not only records but also a model. Indeed, a model enables new means of organizing observations, the discovery of principles and reoccurring structures and, hopefully, the prediction of future events.

### 6.1. Three data dominions

Prediction takes different forms depending on the domain considered. We can identify at least three kinds of dominions that each follow a slightly different logic in their datafication processes:

- The **temporal dominion** deals with predicting periodic rhythms and chronological patterns. Intellectual technologies associated with this dominion included calendars, chronologies, and causal tables. Astronomy and astrological tables aimed at making sense of long-term phenomena using dedicated measuring and recording methods. Today’s large-scale models from climatology, meteorology, and geology are the continuation of these datasets.

- The **natural dominion** addresses understanding and classifying living beings. Intellectual technologies associated with this dominion were typically lists, trees and other classification schemes, and indexes. Contemporary Big Science datafication projects for modeling genomes, brains, and particle physics are the continuation of these efforts.

- The **cultural dominion** encompasses human exchanges, production of artifacts, consumption of goods, migration and urbanization phenomena, etc. Intellectual technologies associated with this dominion were meant to track and monitor fluxes of exchanges through maps and tables and thereby predict cultural phenomena at different time scales. Google, Facebook, Amazon, and Apple are central actors today of the Big Data in this dominion.

### 6.2. From predictive to prescriptive systems

A common characteristic of modelling in these three dominions is the transformation of descriptive systems into not only predictive systems but also prescriptive models, and in turn transforming data acquisition strategies.

#### 6.2.1 Focus: _La Connaissance des Temps_
 - In 1567, following the wish of Ferdinando II de Medici and under the initiative of two pupils from Galileo, the Accademia Fiorentine del Cimento started a systematic measuring campaign across Europe. A network of correspondents performed local meteorological measurements following a standardized protocol and then sent their measurements to Florence, which acted as a central data repository. This sixteenth-century-distributed measurement system quickly led to a new language for describing meteorological phenomena, giving existence to previously invisible data and consequently **shaping other information systems in return**.

 - In 1666, 100 years later, a compendium of their methods was published under the title: _Saggi di naturali esperienze_. Translated in Latin, it became the referential work for the next century.

 - Ten years later, standards and new measurement methods lead to the creation of an annual publication, _La Connaissance des Temps_ in France.
 - The journal created in 1678 and thereafter continued (it is still active), published every year both the scientific state of the art dealing with astronomy, time scales, referential systems, coordinate transformation, and a collection of data measurements for the current year documenting the position of the planets and other temporal and astronomical phenomena.

#### 6.2.2. Classification and naming systems

Likewise, finding efficient and adaptable nomenclature to describe the seemingly infinite diversity of living beings was, for a long time, the cornerstone of the Big Data approach in the natural dominion. The binomial nomenclature formalized by Linné in Systema Naturae was progressively upgraded and adapted at each subsequent edition from 1735 (1st edition) to 1758 (10th edition). The binomial formalization was not entirely new as a classification and naming system.

Gaspard and Johan Bauhin had developed a similar system nearly 200 years earlier. However, Linné used it in a consistent and systematic manner, offering an organizing paradigm for new observations. The 1758 edition featured an organization of 4,400 animals and 7,700 plants and established Linné’s nomenclature as the new paradigm for describing the multitude of species discovered or to be discovered. Reality is getting transformed and being **redescribed in order to create a coherent system**.

#### 6.2.3. Focus: Cerdà’s Barcelona (1859)

- Obviously, it is in the cultural dominion that the prescriptive effect is the strongest. In the nineteenth century, Ildefonso Cerdà, considered to be the inventor of modern urbanism, was the first to transform a data measurement approach about **key urban statistics into a science of the city**.

- His approach identified and measured not only transportation and energy flux but also global characteristics about the general quality of life in a city (sunlight, ventilation), and thereby fostered the design of spatial configurations that optimized these reified concepts. Once again, **the descriptive system became a prescriptive one**.

- For his project of expanding the area of Barcelona in 1859 Cerdà took a network-oriented approach. His pattern-based design made the city into a logical system, optimal from a statistical point of view but also partly adjustable depending on actual measurements of its performance.
- This Big Data-informed design was capable of producing a **predictive and adaptive computational urban model**.

#### 6.2.4. The need for the projections of pattern-based models

As these examples illustrate, regulated representations are not just formal conventional systems for encoding data. They form the basis of prescriptive systems, projecting their inferred pattern-based models to shape particular interpretations and observations. When a regulated representation evolves, the associated observation grid changes in turn.

Complex information systems creates ways of seeing reality which in turn influence our own actions. Identifying and understanding these information systems of the past is essential to grasp the ways people were experiencing the past. Thus becomes apparent the need to reconstruct the models missing.

## 7. Redocumentation processes

> Hypothesis : Data Acceleration Regime produces a reduction of "temporal horizons"

In general terms, massive datafication and data acceleration regimes tend to lead to some new forms of disequilibrium: **There is too much information about the present and only “incompatible” information about the past**. This can lead to a reduction of “temporal horizons,” a shift of “temporal regime,” making long-term thinking difficult.

In these moments, the present becomes “bigger” and prediction about the future more difficult. To be capable of longer term predictions, longer datasets are necessary. **Pressure increases for “redocumenting” the past using the regulated representations of the present.**

### 7.1. Recollecting and remapping


#### 7.1.1. Focus: Matthew Maury’s “The Physical Geography of the Sea (1855)

- Redocumentation processes were frequent in the nineteenth century, when data paradigms and standard of measurement were mature. For instance, Matthew Maury created a standard form for logging ship data (including currents, winds, etc.) and produced an entirely new form of navigational chart.
- His masterwork _The Physical Geography of the Sea_ contained about 1.2 million data points and was based, to a large extent, on the additional “massive” extraction of data from old logbooks—a systematic reinterpretation of older data using a new grid.

#### 7.1.2. Two processes
Translating historical datasets into the structured information of a new paradigm can be challenging when it implies processes of recollection and remapping.

**Recollection**, like any archival practice, implies choosing some data and rejecting others and therefore amplifying certain sources while neglecting others.

**Remapping** implies the bending of data in some ways to fit the new regulated representation, with the risk of introducing artefacts. In most cases, it requires finding homologous points in space and time in order to realign models.

#### 7.1.3. Focus: Alexandria vs Pergamon

- In the Antiquity, methodological wars were already being staged over how linguistic redocumentation processes should be conducted. The debate stemmed from the following question: _What should we do when different sources clash over the content of the same book?_
- Librarians at Alexandria and Librarians at Pergamon are the classical representative of the opposing view of the so-called **analogists** and **anomalists**.
- The collation methodology was practiced by the Alexandrians to reconstruct ideal authoritative texts and a kind of ideal **virtual reconstruction** out of multiple copies. In some cases, Alexandrian librarians and their followers went as far as deciding to reintroduce “ideal” linguistic forms that did not exist in the document, following of _Neoplatonistic_ philosophy in their textual transmission strategy. In essence, they considered their virtual reconstruction more real than the physical document they had.
- On the other side of the debate, Pergaminians held that **the document was more important** and insisted on describing linguistic elements based on the preserved documentary forms, possibly following the _Stoician_ philosophical stance that all material traces are inevitably flawed.
- This debate from antiquity is still at the heart of the creation of linguistic formal systems based on redocumentation processes.

In fact, going beyond lingusitics, the balance between data-driven information and simulation-based models is still debated in many fields (biographical studies, cartography, archeology, etc).

### 7.2. New formalisms and the reshaping of data

Grammatical systems themselves need to remodelled to adapt to a new formal model. In the nineteenth century, the German linguist Franz Bopp rediscovered, before Chomsky (cf. FDH-1-1), the Panini grammar and pioneered early comparative theories about Indo-European languages.

Later, several founders of modern linguistics such as de Saussure, Bloomfield, and Jakobson reinterpreted the ancient grammar into the formal systems they invented from describing languages. In such a context more than in any other, **data is not given but rather constructed through a long chain of _recollection_ and _remapping_**.

#### 7.2.1. Focus: Bamboo Annals (210 BC)

- The so-called Bamboo Annals, written on bamboo slips, a classical writing medium before the Chinese invention of paper, is one of the most ancient chronicles of China, covering the period from the legendary times to the third century BC.
- Discovered in a tomb in 298 Anno Domini (AD), they survived the burning of books (and burying of scholars) that is thought to have occurred in China around 210 BC, which resulted a vast collective memory loss in Chinese History.
- After their discovery and throughout their translation and reinterpretation, the contents were significantly modified, adapting to the new ways of describing events and chronology. Due to this redocumentation process, they feature a unique regularly paced **reconstructed chronology of China’s antiquity**. As such, these annals also invented an original way of organizing historical events following specific geographical and temporal conventions, a kind of ancient information system for the past.
- Adapting this object from the past to our times – by means of remapping – made it much more eloquent and powerful, but at the same time partly deformed the origimal information.

### 7.3. The solution to remapping artefacts: Fixed points
To avoid as much as possible the artefacts induced by the remapping, reference elements are looked for. In the historical and geographical domains, redocumentation implies the existence of fixed points, which are common references that enable the alignment of data produced under different paradigms.

#### 7.3.1. Focus: Venerable Bede and the AD temporal origin

- The Venerable Bede (673–735), an English monk, had to address the problem of aligning different chronological systems (such as ever-changing regnal years, or 15-year periods known as _inductions_) to organize the profusion of unverified historical facts in a common system.
- To address this challenge, Bede defines the AD temporal origin using homological events present in various chronicles. He suggests that, from now on, all events should be dated according to the number of years separating them from the birth of Christ.
- The dissemination of this new dating system played a crucial role for shaping a **common chronological framework** and aligning different historical sources into a **coherent dating system**.

#### 7.3.2. Focus: Ranulf Higden's _Polychronicon_

- The _Polychronicon_, written by Anglo-Saxon Benedictine monk Ranulf Hidgen (1280–1363), is a compilation of several chronicles combining many different traditions that aimed to be an encyclopaedic world history.
- To perform this universal recompilation of knowledge, Hidgen developed a **systematic framework to redocument** the records of other chronicles.
- To align events in a coherent temporal framework, Hidgen use eight calendar systems: three Hebraic (one starting in January, one starting in February, one starting in March), three Greek (Troy, Olympiads, Alexander), one Roman (_Ad Urbe condita_, starting in 753 BC), and a Christian one.
- Events reported were then tagged in the margin of the text using several chronological systems. This multicolumn system was pivotal to the organization of this universal chronicle.


Finding homological events, as Bede and Hidgen did, are key to remapping chronologies in standardized temporal projections.

Likewise, a particular landmark, such as a church’s campanile, could serve as a fixed point to align maps of different centuries, and several common landmarks could constitute the basis of a mapping function to realign data from the past.

In other cases, structural mapping can be envisioned, for instance, for aligning early modern documentary structure with contemporary information systems.


### 7.4. Filling the gaps: Recursive redocumentation

A side effect of redocumentation is that gaps tend to be filled.

Indeed, data from the past systematically undergo a form of regularization to match a new paradigm. During this regularization process, **data are reinterpreted, patterns are induced, and new data are inferred**.

In that sense, redocumenting data from the past is like trying to simulate the past and filling the gaps left by the datasets **using the inference methods of new paradigms**.

The core challenges of redocumentation go beyond the mastering of recollection and remapping techniques. Precisely because redocumentation is not a one-step process but a series of recursive reconstructions, **redocumentation processes must be carefully modelled**. Ideally, recollection and mapping operations should be described along with the persons performing them.

In many contemporary cases, some of these operations include algorithmic steps which should be equally described. The standard for such form of _metahistorical modelling_ or _paradata_ must still be invented and agreed upon.

But in parallel to recursive redocumentation emerges the increasing **difficulty of "unpacking" the history behind a data value** – that is of backtracking the successive changes induced by the series of redocumentations.

Note, however, that this perpetual repurposing of data is not exclusive to cultural datasets and that it is also very much a present-day frequent phenomenon.

## 8. Fictional spaces

### 8.1. Trusting historical information

Information extracted from historical documents can be wrong in many ways. Historical reconstruction is highly uncertain not only because of the long series of redocumentation processes but also for many other potential reasons.

The document **contents could be false or imprecise**. This affects all kinds of primary and secondary sources from ancient manuscripts, administrative records, and cadastral maps to contemporary academic books, news articles, or virtual reconstructions.

In addition, the regulated representations encountered in two documents could be partly incompatible, like the diverse chronology that Bede and Hidgen needed to align. Eventually, any procedural method and particularly contemporary algorithmic processes can add noise, artefacts, and other errors in both the processing of single documents and in the alignments of large sets of extracted information.

Any procedural method and particularly contemporary algorithmic processes can add noise, artefacts, and other errors in both the processing of single documents and in the alignments of large sets of extracted information.

### 8.2. Disjointed fictional spaces

This provides motivation to consider that information extracted from historical documents could be organized in potentially disjointed fictional spaces. **Each fictional space contains only coherent information** extracted from an identified set of sources. It constitutes a virtual historic reconstruction, a possible historic reality, and as such may not be compatible with other fictional spaces – this incompatibility leads to _disjointed_ fictional spaces.

#### 8.2.1. Assessing the coherence of a fictional space

Coherence is calculated by the **non-violation of a series of defined constraints**.

For instance, it could be assumed that a dead person may not interact anymore after the date of his death, or that someone cannot be at two different places at the same time. If one document indicates that Dürer is in Venice in early June 1502, and it can be deduced from another one that he is in Nuremberg at this date, two fictional spaces must be created.

One difficulty is that such common sense constraints may vary depending on beliefs (e.g., the possibility of supernatural forces impacting the world!) and actual knowledge about a given culture (e.g., depending on means of transportation, the subsequent presence of the same person in two different places may or may not be possible – is it possible that Dürer made the travel from Venice to Nuremberg in one day?)

Finding flexible ways of representing such constraints is a core challenge of this kind of modeling.

### 8.3. Joining fictional spaces

Several **fictional sets could be joined to create bigger ones**. If the coherence constraints match, they could be merged into a larger set.

As in any redocumentation process, this implies mapping the sets on a common framework of regulated representations and aligning the two sets by identifying common stable elements (identifying a person, a particular place or a concept as the same). These mapping operations should obviously be fully documented in a standard form as they inform the nature of the reconstruction.

Imposing only a fully compatible union between fictional spaces maybe unrealistic, given the uncertain and incoherent nature of extracted historical information. This could result in large sets of disjointed fictional spaces.

For this reason, **it may be relevant to consider partial matching** and to evaluate it based on a **cost function** proportional to the amount of incoherent information provoked by the matching. Fictional spaces with marginal cost, in terms of incoherence, may be accepted as offering a potentially coherent reconstruction.

Metaphorically, constraints acting over the entire reconstructed system may be compared to a **large system of springs** that generally pushes the system toward a stable solution but allows for a certain degree of freedom. The cost of reconstruction would correspond, in this case, to how much the system must be stressed to fit the desired configuration.

#### 8.3.1. Algorithmic optimisation

The action of algorithms will be crucial to performing the optimized union of fictional spaces. Beyond a given volume of extracted information, **human tests of coherence are out of reach**.

Given that coherence constraints need to be written using formal operators, and since certain historical hypotheses are likely to be expressed using procedural methods, the historical debate is likely to be increasingly mediated by algorithms (just like many proofs in mathematics today).

#### 8.3.2. Large-scale unification engines

The design of large-scale unification engines capable of exploring the various combinations of fictional spaces is one of the greatest challenges to creating a large set of coherent Big Data of the past. Such an effort can be traced back to early expert systems, now rebranded as cognitive computing.

This problem of solving constraints necessitates finding good exploration strategies, and it is likely that in many cases only suboptimal unions can be found. In addition, the arrival of new data from digitization and extraction could create new fictional spaces that are incompatible with the present unification. In such cases, backtracking may be needed to separate previously joined spaces.

#### 8.3.3. Reconstruction “temperature”

One can consider the landscape of all the possible pasts and see the reconstruction as its progressive exploration. The cost of each reconstructed past can be evaluated based on the number of statements it does not consider and the number of constraints it breaks. In such terms, **the unification of fictional spaces can be seen as an optimization problem**, analogous, for instance, to simulated annealing.

To avoid being trapped in local “valleys” due to suboptimal unions of fiction spaces, it could make sense to create disorder, to temporarily increase the level of the reconstruction “temperature”, relaxing some constraints, disjoining some of the fictional spaces and letting other, potentially more fruitful unifications emerge. The methodology and technology to perform such global optimizations has yet to be developed and constitutes a cornerstone of the successful exploitation of Big Data of the past.


## 9. Conclusion
### 9.1. Summary

Big Data of the past can be seen as a construct, articulated on the basis of six intermediary concepts.

- **(1) The bigness of data** (cf. FDH-1-2) can be measured in a least four dimensions. Big Data is big because of the technological difficulty to process it, because of its open-ended nature, because of its networked structure, and because it fosters the structuration of new knowledge paradigms. Defining quantitative measures for these four dimensions, despite their extremely diverse qualitative nature, is one of the challenges to grounding this concept.

- **(2)** The four dimensions of data bigness are obviously **not independent**. During what can be defined as data acceleration regimes, they mutually reinforce themselves in circular dynamics. **Data acceleration regimes** can be spotted not only during the European Renaissance and the transformations of the nineteenth century but also much further in the past with the administrative inventions of Mesopotamian city states or the Roman Empire. Finding rigorous methods to identify and segment these “Big Data moments” spatially and chronologically is one of the challenges to studying the global impact of these information transformations.
- **(3)** At a finer granularity, what links technology with a continuous production of standardized data streams are **regulated representations**, governed by sets of production and usage rules. Indexes of names, accounting tables, family trees, flow-chart diagrams, maps, or graphs are examples of such regulated representations. Mapping the genealogical evolution of regulated representations in space and time and understanding **how to translate and align their contents are two necessary preparatory steps to reconstructing the transformation of data structures** in time.

- **(4)** **Inferred patterns** are the results of the structuration imposed by regulated representations. During each data acceleration regime, regulated representations structures data streams, making it possible to accumulate organized information and to infer subsequent data patterns. Descriptive models turn into prescriptive ones, guiding data acquisition strategies and impacting design and societal choices. For this reason, regulated representations should always be considered in association with the prescriptive patterns they induce. The core challenge is to find forms that express these underlying models and exploit their prescriptive dimensions to compensate for a lack of information about the past.
- **(5)** **Redocumentation processes** have always been needed to translate information stored in obsolete regulated representations into updated information paradigms. Redocumentation implies **recollecting** (i.e., choosing what data to keep and what data to ignore) and **remapping** (i.e., searching for homologous points in space, time, and other more complex cultural dimensions). As redocumentation is a central characteristic of the nature of the dataset from the past, it is crucial to not only understand how to redocument old datasets using the paradigms of the present but how successive translation processes previously occurred in the past, explaining the particular nature of the data considered. As Big Data of the past is the result of a series of rewriting processes, the core challenge is not only to perform these redocumentations but to model them as recursive operations.
- **(6) Merging fictional spaces**  – Information about the past that results from different datafication operations and consecutive redocumentation processes is always potentially unreliable and often inconsistent. Coherent sets of information extracted from historical documents can be organized as disjointed but locally consistent fictional spaces. The core challenge is to unify these **fictional spaces** into larger spaces, sometimes accepting partial inconsistencies. Through this formalization, historical reconstruction becomes a problem of optimization.

### 9.2. In the next chapter

In the next chapter, we will study the dynamics of growth of Big Cultural Datasets and show examples of control struggles through the study of patrimonial capitalism.

## Further Reading

- This chapter is largely based on : Kaplan, di Lenardo (2017) Big Data of the Past

- Simondon

- Pascal Robert

- Harmut Rosa
- Ann Blair, Too Much too know, Yale

- Bolter J.D., Grusin R. Remediation: Understanding New Media, Cambridge, MA, 2000 MIT Press (p.60)

Each new medium is justified because it fills a lack or repairs a fault in its predecessor, because it fulfills the unkept promise of an older medium’
