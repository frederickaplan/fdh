---
title: 'Foundations in Digital Humanities 1.2'
subtitle: 'Digital Humanities as a structured research field'
author:
 - Frederic Kaplan

# Don't change the following lines
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyfoot[L]{-release-version-}
output: pdf_document

---

# Digital Humanities as a structured research field

## Digital Humanities constantly define themselves dynamically out of structuring tensions.

Defining the nature and the boundaries of digital humanities is a long-discussed and unsolved issue, not only because there is no consensus on this question but also because digital humanities are currently undergoing a profound transformation that calls for a reconsideration of its fundamental concepts. For years, digital humanities have been loosely regrouping computational approaches of humanities research problems and critical reflections of the effects of digital technologies on culture and knowledge. They emerged as a new label, rebranding and enlarging the idea of “humanities computing”.  Around this new name and under a “big tent,” a progressively larger community of practice thrived. Each work at the intersection of Computer Science and the Humanities could potentially be part of this welcoming trend. Researchers gathered in national and international meetings, exchanged their views on blogs and mailing lists. If not a well-bounded field, digital humanities were surely a lively conversation.

The welcoming digital humanities label opened doors, connected separated academic silos, built bridges between information sciences and the various disciplines loosely forming what is called the humanities. However, openness was always associated with a need for introspection, self-reflexive writings, tentative boundaries definitions, the “What are digital humanities” articles and monographs became a genre of its own structured around several narratives of exclusion and inclusion. Digital humanities as a research domain define themselves dynamically in the negotiation of these tensions. 

- Humanists vs. Digital Humanists : When does research in the humanities become digital humanities ? Is it enough for humanities scholar to have a website to become a digital humanist ? Does the use of a computer in humanities research make digital humanities research ?
- Computer scientists vs. humanists in DH: Should we still distinguish computer scientists and humanists in digital humanities communities ? Is the “two cultures” tension still relevant ? Are digital humanities a forme of “technical upgrade” of the humanities disciplines ? Are digital humanities just a particular “application” of the Computer Science fields?
- Makers vs. Interpreters: Are digital humanities only about “building things” ? If you are not a “maker”, should you still be considered a “digital humanist” ? Is there room for purely interpretative digital humanities ?
- Distant readers vs. Close readers: Are Digital Humanities only about “Distant Reading” ? To study literature, should we stop reading books and only focus on quantitative algorithmic measures ? Can digital humanities also enhance close reading experience ? Are “distant trading” approaches a form of radical digital humanities ?
- Big Data DH vs. Small Data DH: Research in Big Data Digital Humanities focuses on large or dense cultural datasets, which call for new processing and interpretation methods. Big Data are “big” when, for instance, “manual” analysis becomes cumbersome and new study and interpretation methods must be invented. It is important to note that Massiveness of Big Data is not tightly linked to a certain number of Terabytes, but with more subtle characteristics we will shortly see. In comparison, the Small Data Digital Humanities regroup more focused works that do not use massive data processing methods and explore other interdisciplinary dimensions linking computer science and humanities research. Small Data are small in the sense that it is not only smaller-scale but also well-bounded.We will argue that Big Data research in digital humanities can be characterized by common methodologies and objects of studies, therefore transcending some of the tensions that have structured digital humanities so far.

## Digital Humanities deal with Big Cultural Datasets

Massive cultural digital objects include large-scale corpus like the millions of books scanned by Google and the ones produced by numerous other digitization initiatives, the millions of photos and micro-message shared on social network services, giant geographical information systems like Google Earth, or the ever expanding networks of academic papers citing one another . These interconnected objects – either digitally born or reconstructed through digitization pipelines – are too big to be read or watched. The traditional 1:1 ratio of a single scholar confronted with one document cannot cope with such abundance. Moreover, their boundaries are sometimes fuzzy, their content partially unknown and, likely to be in continuous expansion. These characteristics make them profoundly different from corpora traditionally studied by humanities researchers, despite surface resemblances.

The confrontation with these “massive” objects calls for fundamental questions. What can really be extracted from these huge datasets and what interpretations can be drawn based on these extractions? Will we learn more by analyzing 10 millions books that we cannot read individually or by reading five carefully? What is the role of algorithms for mining, shaping, and representing these large digital objects?

In order to understand the specifify of Big Cultual Dataset we should understand what Big Data are and what are the particulirites of Cultural Datasets in this respect. 

## Big Data is structured around two narratives

The term Big Data is associated with two foundational narratives, both of which present it as an epochal paradigm shift. In the Data Deluge narrative, Big Data is a reaction to an unexpected abundance of information. In the Big Science narrative, Big Data is a structured effort by the international scientific community to crack very hard problems by joining research forces and creating large-scale infrastructures. Both narratives contribute to structuring a multifaceted definition of the bigness of Big Data.

In the data deluge narrative, Big Data is born out of the possibilities of the Internet and digital communication networks. In the last decade, several companies and research groups realized that fluxes of real-time information that irrigate endlessly growing worldwide information systems have the potential to constitute an original knowledge base for understanding the present and anticipating the future. From that perspective, Big Data research essentially tries to convert these fluxes into structured knowledge systems that document the lives of people, companies and institutions, aggregating information about places, topics, or events. The resulting knowledge systems are coded in machine-readable formats, facilitating data mining and exchanges. This general transformative process has been called \*datafication\*. In such a perspective, Big Data research is the science behind massive datafication.

Interest in Big Data has another—slightly older—origin that is connected to the constitution and management of very large scientific archives, what has been called Big Science. In this narrative Big Data, methodologies were initially pioneered in some domains of life sciences, climatology, astronomy, and physics. The Human Genome Project, starting with its creation in 1989 to its achieved target in 2001, paved the way for large-scale collaborative research infrastructures and experimented in publishing the resulting data sets and results. The massive data produced by CERN required the construction of new software and hardware systems. International attempts to model and simulate the brain based on massive curated experimental data revealed new challenges in the link between measures and simulation. The Big Science narrative insists on tackling the challenge of organizing collaborations on an international level despite academic competition, designing information pipelines to harness the massiveness of the data produced and on the relevant use of algorithmic simulation to test the coherence of the data and extrapolate for making new predictions.

## Data Bigness has four dimensions

Technology: Big Data is big in the sense that it “hurts” to compute it using traditional “manual” methods. Its bigness calls for new strategies of processing and interpretation.The “envisioned” data volume needs special storage and computing infrastructure to be managed. Such data-intensive computing infrastructures include, for instance, large clusters and parallelization algorithms. In turn, such technological progress opens the way for storing and computing even more data.

Open-edness : Big Data is big when it is in a state of continuous open-ended expansion. Large-scale databases of book scans are in perpetual extension, photos uploaded on social networks constitute ever-growing datasets, and the volume of micro-messages sent per day keeps rising. This calls for iterative methodologies, different from the ones adapted to close datasets. From the perspective of Big Data, every dataset tends to become a data stream (i.e., a part of the data deluge).

Relational : Data is big not only because of its size but because of its relationship with other data and its “fundamentally networked” nature.The semantic web approach that hypothesizes the existence of a Giant Global Graph, a machine-readable version of the information contained on the World Wide Web, is a typical example of such kind of interlinked datasets.

Paradigmatic : Big Data is big when there is sufficient data to perform new forms of data-driven sciences. It is currently being debated how massive research into patterns or correlations in large databases could replace hypothetic-deductive and model-based approaches. For instance, by relying on massive amount of examples, translations could potentially be done without any grammatical models, and species could be identified by their genomic signature without knowing much biology.

## Digital Humanities is the field dealing with Big Cultural Datasets

Main proposal : (Big Data) Digital Humanities deal with Big Cultural Datasets. Cultural Datasets are Big either because they are hard to processs, open-ended, fundamentally networked or sufficiently dense to lead to data-driven approach. Big Cultural Datasets are specific objects of studies as neither Computer Science nor the Humanities know how to deal with these large objects. Digital Humanities are a field (and not just an intersection zone) for this reason. 

## Research in Digital Humanities can be organised in the three concentric circles

### First Circle

The first circle corresponds to research focusing on processing and interpretation big and networked cultural data sets.For instance, each photo uploaded to Facebook is processed in series of consecutive steps, including machine learning analysis, face recognition, etc. The study of this ever expanding corpus needs new specific methods.

The particularly complex nature of Cultural Datasets makes these processing specific compare to standard Data Science methods. We will see in details the nature of the complexity in the next course which is focusing on “historic" Big Cultural Datasets (Big Data of the Past)

### Second Circle

It is important to consider that data processing and interpretation occur in a larger context of the new digital culture characterized by collective discourses, large community, ubiquitous software, and global IT actors. Understanding the relations between these entities could be considered the second object of study for Big Data Digital Humanitie, what we call here, the second circle. 

Consider again the millions of photos shared every hour on Facebook. Large-scale communities produce both the massive digital objects and the collective discourses about massive digital objects. They do so through the mediation of algorithms produced by one giant IT company of the web. Retroactively, collective discourses about the photos have a shaping role on the emergence and structuration of these communities. In addition, as collective discourses reach rapidly a critical mass (e.g., millions of messages or status update) they tend to become themselves massive digital objects, to be archived and studied through specific text and data mining approaches. Understanding photo sharing implies understanding the complexity of this network of interactions.

The processing domain (1) covers the interaction between the software and massive digital object from a technical and epistemological perspective, studying in particular how to design data-processing algorithms capable of deriving new data out of massive digital objects.This corresponds to the first circle.

The discursive domain (2) covers the study of the shape of collective discourses in relation with digital cultural objects, from Facebook to scientific articles.Discourse is not limited to textual domain but also includes video productions. Specific algorithmic methods must be invented to observe in time and space the continuous evolution of such discourses. 

The social shaping domain (3) studies how large-scale communities shape and are shaped by the collective discourses they produce.This includes polarisation dynamics on Twitter or YouTube but also communities formations in other more specific communities (Wikipedia, gamers, developers, etc.).

The algorithmic mediation domain (4) studies how software mediates discourses and communities. This includes the specific effects of sorting algorithms, filter bubbles, scoring systems but also platform effects in Education and Science.

The control domain covers the relationship of communities and global actors with massive digital objects and the software medium. The domain studies how global actors curate both big cultural datasets and software medium to process them or how symmetrically, large-scale communities create or use software infrastructure for instance in the context of open-source communities.  

### Third Circle

Big cultural data, and digital culture at large, are experienced in the real world through physical interfaces, websites and installations. They produce “experiences.” This third circle is an area of study on its own.

## Hard vs Soft definition of Digital Humanities

Soft Definition : Digital Humanities constitute an interdisciplinary field at the intersection of Humanities and Digital approaches. Digital Humanities will remain a diaphragm zone.

Hard Definition : Digital Humanities constitue a field of its own, studying a new kind of objects : Big Cultural Data Sets. Digital Humanities will be a new independent domain. 

## Summary

It can be argued that (Big Data) Digital Humanities study a new kind of object : Big Cultural Data Sets.

Neither Computer Science nor Traditional Humanities know how to deal with it.

As a consequence, some consider that Research in Digital Humanities is a field on its own, organising a restructuration of knowledge based on new domain of investigations focusing of processing, discursive, social shaping, algorithmic mediation and control relations. In this perspective, Digital Humanities also study the experiences produced by these relations and the new interfaces mediate them. 

This can be considered a hard definition of Digital Humanities.  

## In the next Chapter

Digital Humanities deals with Big Cultural Datasets like Wikipedias or Facebook photo’s archives. Do such datasets exist for the past ? Beyond Big Data of the present, can we talk of Big of the Past ?

## Further Reading

Jacquesson 2010 on Google Books. 

Datafication

