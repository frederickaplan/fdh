---
title: 'Foundations in Digital Humanities 1.2'
subtitle: 'Digital Humanities as a structured research field'
author:
 - Frederic Kaplan

# Don't change the following lines
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyfoot[L]{-release-version-}
output: pdf_document

---

# Digital Humanities as a structured research field

## **Digital Humanities constantly define themselves dynamically out of structuring tensions.** 

Defining the nature and the boundaries of digital humanities is a long-discussed and unsolved issue, not only because there is no consensus on this question but also because digital humanities are currently undergoing a profound transformation that calls for a reconsideration of its fundamental concepts. For years, digital humanities have been loosely regrouping computational approaches of humanities research problems and critical reflections of the effects of digital technologies on culture and knowledge. They emerged as a new label, rebranding and enlarging the idea of “humanities computing”.  Around this new name and under a “big tent,” a progressively larger community of practice thrived. Each work at the intersection of Computer Science and the Humanities could potentially be part of this welcoming trend. Researchers gathered in national and international meetings, exchanged their views on blogs and mailing lists. If not a well-bounded field, digital humanities were surely a lively conversation.

The welcoming digital humanities label opened doors, connected separated academic silos, built bridges between information sciences and the various disciplines loosely forming what is called the humanities. However, openness was always associated with a need for introspection, self-reflexive writings, tentative boundaries definitions, the “What are digital humanities” articles and monographs became a genre of its own structured around several narratives of exclusion and inclusion. Digital humanities as a research domain define themselves dynamically in the negotiation of these tensions. 

- Humanists vs. Digital Humanists : When does research in the humanities become digital humanities ? Is it enough for humanities scholar to have a website to become a digital humanist ? Does the use of a computer in humanities research make digital humanities research ?
- Computer scientists vs. humanists in DH: Should we still distinguish computer scientists and humanists in digital humanities communities ? Is the “two cultures” tension still relevant ? Are digital humanities a forme of “technical upgrade” of the humanities disciplines ? Are digital humanities just a particular “application” of the Computer Science fields?
- Makers vs. Interpreters: Are digital humanities only about “building things” ? If you are not a “maker”, should you still be considered a “digital humanist” ? Is there room for purely interpretative digital humanities ?
- Distant readers vs. Close readers: Are Digital Humanities only about “Distant Reading” ? To study literature, should we stop reading books and only focus on quantitative algorithmic measures ? Can digital humanities also enhance close reading experience ? Are “distant trading” approaches a form of radical digital humanities ?
- Big Data DH vs. Small Data DH: Research in Big Data Digital Humanities focuses on large or dense cultural datasets, which call for new processing and interpretation methods. Big Data are “big” when, for instance, “manual” analysis becomes cumbersome and new study and interpretation methods must be invented. It is important to note that Massiveness of Big Data is not tightly linked to a certain number of Terabytes, but with more subtle characteristics we will shortly see. In comparison, the Small Data Digital Humanities regroup more focused works that do not use massive data processing methods and explore other interdisciplinary dimensions linking computer science and humanities research. Small Data are small in the sense that it is not only smaller-scale but also well-bounded.We will argue that Big Data research in digital humanities can be characterized by common methodologies and objects of studies, therefore transcending some of the tensions that have structured digital humanities so far.

## Digital Humanities deal with Big Cultural Datasets

Massive cultural digital objects include large-scale corpus like the millions of books scanned by Google and the ones produced by numerous other digitization initiatives, the millions of photos and micro-message shared on social network services, giant geographical information systems like Google Earth, or the ever expanding networks of academic papers citing one another . These interconnected objects – either digitally born or reconstructed through digitization pipelines – are too big to be read or watched. The traditional 1:1 ratio of a single scholar confronted with one document cannot cope with such abundance. Moreover, their boundaries are sometimes fuzzy, their content partially unknown and, likely to be in continuous expansion. These characteristics make them profoundly different from corpora traditionally studied by humanities researchers, despite surface resemblances.

The confrontation with these “massive” objects calls for fundamental questions. What can really be extracted from these huge datasets and what interpretations can be drawn based on these extractions? Will we learn more by analyzing 10 millions books that we cannot read individually or by reading five carefully? What is the role of algorithms for mining, shaping, and representing these large digital objects?

In order to understand the specifify of Big Cultual Dataset we should understand what Big Data are and what are the particulirites of Cultural Datasets in this respect. 

## Big Data is structured around two narratives

The term Big Data is associated with two foundational narratives, both of which present it as an epochal paradigm shift. In the Data Deluge narrative, Big Data is a reaction to an unexpected abundance of information. In the Big Science narrative, Big Data is a structured effort by the international scientific community to crack very hard problems by joining research forces and creating large-scale infrastructures. Both narratives contribute to structuring a multifaceted definition of the bigness of Big Data.

In the data deluge narrative, Big Data is born out of the possibilities of the Internet and digital communication networks. In the last decade, several companies and research groups realized that fluxes of real-time information that irrigate endlessly growing worldwide information systems have the potential to constitute an original knowledge base for understanding the present and anticipating the future. From that perspective, Big Data research essentially tries to convert these fluxes into structured knowledge systems that document the lives of people, companies and institutions, aggregating information about places, topics, or events. The resulting knowledge systems are coded in machine-readable formats, facilitating data mining and exchanges. This general transformative process has been called \*datafication\*. In such a perspective, Big Data research is the science behind massive datafication.

Interest in Big Data has another—slightly older—origin that is connected to the constitution and management of very large scientific archives, what has been called Big Science. In this narrative Big Data, methodologies were initially pioneered in some domains of life sciences, climatology, astronomy, and physics. The Human Genome Project, starting with its creation in 1989 to its achieved target in 2001, paved the way for large-scale collaborative research infrastructures and experimented in publishing the resulting data sets and results. The massive data produced by CERN required the construction of new software and hardware systems. International attempts to model and simulate the brain based on massive curated experimental data revealed new challenges in the link between measures and simulation. The Big Science narrative insists on tackling the challenge of organizing collaborations on an international level despite academic competition, designing information pipelines to harness the massiveness of the data produced and on the relevant use of algorithmic simulation to test the coherence of the data and extrapolate for making new predictions.

## Data Bigness has four dimensions



## Summary



## Further Reading

Jacquesson 2010 on Google Books. 

Datafication

