---
title: 'Foundations in Digital Humanities 1.6'
subtitle: 'Anatomy of a large-scale project'
author:
 - Frederic Kaplan

# Don't change the following lines
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyfoot[L]{-release-version-}
output: pdf_document

---

# FDH-1-6: Anatomy of a large-scale Digital Humanities project

#### Core theses explored in this chapter

-  Large scale project could be built based on open technology and data commons
-  The key is to demonstrate the relevance locally and then extend globally

This chapter illustrates the various notions developed in the previous chapter with an integrated example, the development of the **_Venice Time Machine_** and its extension to Europe.

#### Contents
>1. Chronology of the Venice Time Machine
>2. Time Machine Europe
>> 2.1. Digitisation Infrastructure // 2.2. Local Time Machines // 2.3. The RFC process // 2.4. Towards the Mirror World //
> 3. Conclusion
>> 3.1. Summary // 3.2. In the next chapter //

## 1. Chronology of the Venice Time Machine

_The Venice Time Machine's bootstrapping phase was developed between 2012 and 2019. The project is now in its second phase of development, planning activities from 2020 to 2028._

**2013**

The Venice Time Machine project officially begins with an initial agreement signed on February 23, 2013 between Ecole Polytechnique Fédérale de Lausanne (EPFL) and Ca ’Foscari University. For this signing, the Italian Minister of Education and Research, Francesco Profumo, as well as the Swiss Secretary of State for Education, Research and Innovation, Mauro Dell’Ambrogio, make the trip to Venice, thus highlighting the collaboration within the context of good Switzerland-Italy relations.

A joint training program between EPFL and Ca’Foscari University is set up, taking the form of regular autumn schools: joint activity weeks were organized by the partners. The objective of these training courses, attended not only by students from EPFL and Ca’Foscari University but also young researchers from several other European and American institutions, is to develop an interdisciplinary training around archival material and new technologies.

**2014**

The State Archives, Ca’Foscari University and EPFL sign a first formal collaboration document framing a joint program of actions for the future. The aim of the project is to transform the documentary heritage of archives into an online information system which is available online for the community of researchers and specialists, but also for the general public. The agreement specifies that “t*he digitization of ancient documents is an essential step for the conservation and enhancement of cultural heritage, two of the fundamental missions of archives.*” And that “*digital images .. make research possible worldwide allowing thus the creation of ambitious international projects.*”

The text continues: “*For these projects to be carried out, it is important to create a freely accessible database of images of documents associated with related instruments and records of archival descriptions.*” Finally, to avoid any ambiguity, the agreement specifies: “*In addition to viewing the images, it will be possible to download them in accordance with standards of the Code of Cultural Property and Landscape*“.

The objective of the project is described as “*helping the Venice State Archives to make rapid progress in the digitization of the documents it stores and in making these documents available to the international research community.*” It is for this reason that the images will be “*distributed globally with an open license.*”

The EPFL provides the scanners, servers, computers and all the necessary equipment for the creation of a first digitization space to be installed within the State Archives of Venice. A pre-study phase in close collaboration with the archivists takes place from June to September 2014 in order to test the performance of the digitization chain, primarily in terms of speed in accordance with the categories of documents considered. On the basis of this preliminary study, the choice of series and the configuration of the teams would be established. On June 2014, the official inauguration of the digitization center takes place in the presence of Patrick Aebischer (President of EPFL), Carlo Carraro (Rector of Ca’Foscari), Raffaele Santoro (Director of ASVe) and Thierry Lombard (main sponsor of the project).

EPFL hires and trains five Italian specialists for the operations of the scanners and the annotation of digitized documents, as well as a qualified team leader with a paleography and archival background (trained by the school of archival and internal paleography in the Archive): Fabio Bortoluzzi. Among this team, one of the archivists will join the State Archives of Venice a few years later, and Fabio Bortoluzzi himself will go on to become director of the Vicenza State Archives.

**2015**

The protocol for digitization, metadation and annotation is established by the team leader and the archivists of the Venice State Archives on the basis of the results of 2014’s pre-study phase. An estimate of the number of hours is produced for the digitization and description of various documents series. On the basis of these estimates, it is decided to carry out a basic description of the registers and to concentrate efforts on searches facilitating the automatic extraction of information. In July 2015, a first version of an annotation software is deployed within the State Archives.

2015 is also a period of intensification of the collaboration between the EPFL teams and the other Venetian institutions. Several parallel projects are launched. The [“Garzoni” project](https://www.epfl.ch/labs/dhlab/projects/garzoni/) – a partnership between EPFL, the University of Lille (Valentina Sapienza) and the University of Rouen (Anna Bellavitis), funded by the Swiss National Fund and The French National Agency for Research – aims to build an information system in order to conduct historical research on the question of learning from the perspectives of the economy, family, gender, art and architecture. It focuses on the “Gustizia Vecchia” collections, which had already been digitized by the University of Lille in partnership with the State Archives, and is coordinated by Maud Ehrmann for EPFL, involving a dozen other researchers.

A second project funded by the Swiss National Science Foundation, [Linked Books](https://www.epfl.ch/labs/dhlab/projects/linkedbooks/), begins on September 1, 2015. The project explores the “history of history” of Venice using new algorithmic approaches, based on networks of citations and full-text analyzes of publications. The project is coordinated by Giovanni Colavizza and Matteo Romanello and concerns a corpus of more than 2,000 monographs and 5,000 newspaper articles published over the past 200 years and dealing with all aspects of Venetian history. For this project, several specific contracts are established to supervise the digitization of the collections of secondary sources necessary for the project, notably with the Marciana Library, the Istituto Veneto and the Ca’ Foscari University Library.

Finally, also in 2015, EPFL and the Giorgio Cini Foundation signed an agreement for the launch of the [Replica project](https://www.epfl.ch/labs/dhlab/projects/replica/), coordinated by Isabella di Lenardo which aims to digitize the foundation’s photo library (one million images) and to build an engine for research enabling the search for morphological patterns. The agreement specifies that the digital photo library and the search engine will be open access. A new type of scanner is developed by Adam Lowe’s team at Factum Arte. It is designed as a rotary table that moves continuously during a scanning session, simultaneously photographing both sides of documents on a page and automatically uploading the images to a computer. The project will also give rise to a doctoral thesis by Benoit Seguin who will propose a new way to train neural networks using deep learning to detect recurrences of patterns on media as diverse as drawings, paintings, engravings or photographs.

**2016**

EPFL begins its participation in the READ project in January 2016 to accelerate progress in the handwriting recognition. Venice Time Machine is one of the large-scale demonstrators of the project.

On the EPFL campus in Lausanne, a new building designed by Kengo Kuma, Artlab, is inaugurated in November. A permanent exhibition on the Venice Time Machine is presented in the “Datasquare” pavilion. Director Raffaele Santoro is interviewed several times and his explanations are presented on the pavilion screens, along with other testimonials from historians and researchers working on the project.

**2017**

In 2017, EPFL makes the problem of sharing images via the network more effective with the creation of a first version of the Time Machine Box. It is a server, installed at the location of scanning, that is to say directly at the archives, on which all the scanned documents and their metadata are hosted and easily accessible via the IIIF protocol, which defines international standards on image exchange. The Time Machine Box is not a simple storage space. It allows any research organization to perform an analysis on the images present to perform an analysis using document segmentation algorithms or handwriting recognition, presuming these are compatible with the IIIF standard.

In October of the same year, EPFL, Ca’ Foscari University, the State Archives of Venice and the Giorgio Cini Foundation, publish a joint press release which will give rise to several articles publicly announcing the first results of the project and of the digitization campaign, including: 190,000 digitizations of archival documents, 720,000 photographic documents, 3,000 books covering 200 years of Venetian historiography, making a total of more than 2 million digitized images. On this basis, 160,000 manual transcriptions of name, location and keywords were performed by archivists. A search engine using a handwriting recognition system based on these annotations is announced. To mark this occasion, Michele Bugliesi, the Rector of Ca’ Foscari University declares:

_“The digitization of archival holdings opens up new avenues for the study and understanding of the history of the cultural evolution of past and contemporary civilizations. With this project, Venice is at the forefront of Europe, demonstrating the enormous potential that digital technologies offer for the enhancement of cultural heritage and their ability to develop research methods in the fields of history, art history and more generally for research in the humanities and socio-economic sciences.”_

**2018**

In June 2018, a joint research center established between the Cini Foundation, Factum Arte and the DHLAB of EPFL is inaugurated. The center is named [ARCHiVe – Analysis and Recording of Cultural Heritage](https://www.cini.it/en/institutes-and-centres/archive-analysis-and-recording-of-cultural-heritage-in-venice) in Venice and is funded by the Helen Hamlyn Trust.

As planned, an automatic handwriting recognition system is developed from the annotated Venetian documents within the framework of the European READ project. The results obtained by researcher Sofia Ares Oliveira at EPFL are very encouraging: the recognition performance of this system exceeds the reading skills of an Italian person without archival training. The system is presented for the first time in Mexico City at the Digital Humanities 2018 conference. The same summer, a generic document segmentation system (dhSegment), initially developed to solve the segmentation problem of Replica project, is also made available open source. In just a few months, this free and open system will be used by dozens of archives around the world, including the National Archives in Paris.

The search engine, announced in 2017, combining text search, visual search and geo-historical navigation to allow efficient access to the sources of the Venice State Archive and the Cini Foundation, is unveiled to public during the Time Machine 2018 conference. Indeed, EPFL and Ca’ Foscari University become founding members of a project submitted to the European Commission for the establishment of a “**_European Time Machine_**”, along with 31 other European institutions. Thanks to the extraction methodologies and open technologies developed, the Venetian model can now be exported as a generic format to understand the past of European cities.

A large exhibition at the Venice Biennale of Architecture presents the project in the Padiglione Venezia.

**2019**

The pan-European Horizon 2020 Time Machine Coordination and Support Action is funded by the European Commission. The number of supporting partners continues to grow and reaches more than 400 institutions, confronting Europe’s challenge to build an open database of information that has thus far been segmented into silos. The Venice Time Machine now becomes one among 20 others Local Time Machines.

EPFL wins the Parcels of Venice project to continue research on Computing methodology to extract information from cadastral sources.

## 2. Time Machine Europe

The key startegy of Time Machine Europe is to develop a governance by collectively nurturing Data commons. It is described in a collective written document presenting a 10y strategy for Europe. The content of the following section is based on this document.

### 2.1. Digitisation Infrastructure

The Time Machine digitisation infrastructure is composed of a network of digitisation hubs and will be organised on a European scale. A peer-to-peer platform will be in charge of managing and optimising digitisation strategies at European level, and will also be tasked with the development of generic solutions for archiving, directly documenting the digitisation processes, and swiftly putting the digitised documents online. The hubs will cover regional digitisation needs with standardised hardware for digitisation, storage, information exchanges and on-demand scanning, based on results of Pillar 1 and existing metadata standards, like the one developed by Europeana.

The peer-to-peer platform will federate system integrators at European level, facilitating the deployment of this equipment. The effort will build upon existing EU Research Infrastructures (DARIAH, CLARIN) and infrastructures providing access to cultural heritage (Europeana, Archive Portal Europe, etc.). TM will introduce new processing pipelines for transforming and integrating cultural heritage data in such infrastructures.

Documents are digitised using different kinds of acquisition machines and treated separately depending on their nature (textual and audio-visual documents, iconographic elements, maps, 3D objects and environments). Information is extracted progressively, manually or automatically, to produce elementary historical units, connected with one another. This progressive decomposition and refinement needs to be seen not as a mere automatic process but as a collective negotiation. Each intervention, either algorithmic or human, will be fully traceable and reversible. The results of the processing constitute the core dataset of the Big Data of the Past.

The Time Machine processing infrastructure is composed of a digital content processor and three simulation engines: a 4D simulator, a large-scale inference engine and a universal representation engine.

- The 4D Simulator manages a continuous spatiotemporal simulation of all possible pasts and futures that are compatible with the data.
-  The Universal Representation Engine manages a multidimensional representation space resulting from the integration of the pattern of extremely diverse types of digital cultural artefacts (text, images, videos, 3D), and permitting new types of data generation based on transmodal pattern understanding.
- The Large-Scale Inference Engine is capable of inferring the consequences of chaining any information in the database. This permits to induce new logical consequences of existing data. The Large-Scale Inference Engine is used to shape and to assess the coherence of the 4D simulations based on human-understandable concepts and constraints.

All functions of the different components can be deployed through a fully distributed solution using a storage and computation architecture aimed at an integrated, long-term and sustainable storage of the processed content. This solution embodies our strategy for the long-term availability of processed content, even beyond the lifetime of the organisations hosting it, through predefined and legally binding agreements on licensing, redundant storage, automatic hand-over policies and long-term self-supporting investment initiatives to indefinitely extend the availability of the digitised content of Time Machine.

### 2.2. Local Time Machines

In order to build a planetary scale Time Machine, it is necessary to define an organic incremental strategy. To succeed, the Time Machine must enable to progressively anchor itself in local territories, directly bringing locally higher value to the  activities, favouring the creation of new projects to mine information about the past in surviving objects and documents.
Local Time Machines can be defined as zones of higher density of activities of past reconstruction. The Time Machine Network is organised as an unlimited amount of **Local Time Machines** (LTMs).

### 2.3. The RFC process

Reaching consensus on the technology options to pursue in a programme as large as Time Machine is a complex issue. To ensure open development and evaluation of work, a process inspired by the **Request for Comments** (RFC) that was used for the development of the Internet protocol is being adapted to the needs of Time Machine. Time Machine Requests for Comments are freely accessible publications, identified with a unique ID, that constitute the main process for establishing rules, recommendations and core architectural choices for Time Machine components.

### 2.4. Towards the Mirror World

After the creation of the web that digitised information and knowledge and the social media that digitised people and characteristics of human behaviour, a third technology platform is being created, digitising all other aspects of our world, giving birth to a digital information “overlay” over the physical world, a “**_mirror-world_**”.

The mirror-world will aim to be an up-to-date model of the world as it is, as it was and as it will be. All objects (including representations of landscapes) of the mirror-world will be machine-readable, and, therefore, searchable, traceable and subject to be part of simulations by powerful algorithms. In the mirror world, time will be a fourth dimension, as it will be very easy to go back to the past, at any location, reverting to a previous version kept in the log. One may also travel in the other direction, as future versions of a place can be artificially created based on all information that can be anticipated about the predictable future. Such time-trips will have an increased sense of reality, as they will be based on a full-scale representation of the present world.

Like the other two platforms, the mirror-world will disrupt most forms of human activity, as we know them today, giving birth to an unimaginable number of new ideas (and many problems) and creating new forms of prosperity from new forms of economic and social activity that will shape new behaviours and ecosystems. In this scenario that is currently unfolding, Time Machine will enable Europe to be one of the leading players, shaping the mirror-world according to its democratic values and fundamental ethics (open standards, interoperability). With Time Machine, while it will have a powerful tool to strengthen its cohesion and sense of belonging, Europe has, moreover, an opportunity to impose its own terms against the multinational technology giants that will fight for dominating this new technology platform, just as those who now govern the first two platforms have done in the past.

In the mirror world approach, **each city will have a 3D digital twin**. This machine-readable version of the city will be annotated digitally, thus permitting to create a direct link between the digital information currently on the web or any social network platform to the digital copy of the city itself. The relevant information is attached to each building, shop, metro station, door and other urban infrastructure. As the city’s structure and shape continuously change in time, the city’s digital twin is necessary a 4D model.

In the Time Machine approach, each Local Time Machine ambitions to build a dense database of spatiotemporal information laying the foundation of a 4D model. The Time Machine Organisation helps the city in this process by providing technology, methodology and supporting infrastructure facilitating the digitisation pipelines, the standardisation of the information gathered and the development of related services. All these features are provided by the Time Machine Box, delivered to the institutions participating to the Local Time Machine.

To develop the various exploitation platforms, Time Machine organises the research on novel Human-Computer interaction and visualisation, in particular developing new user-centred 4D interfaces and technology for VR/AR and mixed reality. The different parts of the global Time Machine ecosystem creates self-reinforcing dynamics leading to always denser Big Data of the Past.

## 3. Conclusion
### 3.1. Summary

- The evolution of the Time Machine project from Venice to Europe shows a possible way to create large-scale Data commons.
- The strategy is based on local to global approaches and self-reinforcing dynamics.

### 3.2. In the next chapter

We start the second part of the course on Pipelines.  

## Further Reading
