---
title: 'Foundations in Digital Humanities 3.4'
subtitle: 'Knowledge Modelling'
author:
 - Frederic Kaplan

# Don't change the following lines
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyfoot[L]{-release-version-}
output: pdf_document

---

# Knowledge Modelling

## Concepts

The beauty of Knowledge modelling. Tables. Databases. Semantic web, Ontologies, URI, RDF, CIDOC-CRM, How to code event, places and influence. Metaknowledge. The Hypergraph



Detection of causal chains. Inference of Graphs



## Practice

Graph writing. Presentation of some interesting ontologies: SKOS, VIAF, Geonames, TGN, W3C Time Ontology. SPARQL and SPARQL endpoint. Exercice on SPARQL endpoints: DBPedia [[1\]](http://dbpedia.org/sparql), Talk of Europe [[2\]](http://linkedpolitics.ops.few.vu.nl/yasgui/index.html), Pers√©e [[3\]](http://data.persee.fr/explorer/), Le Temps ARchive [[4\]](http://iccluster052.iccluster.epfl.ch:8899/sparql), available on [this Github repository](https://github.com/dhlab-epfl/fdh-tutorials). (

LOAD model (Andrea Spitz)

HDT http://www.rdfhdt.org/what-is-hdt/

"Currently RDF data is stored and sent in very verbose textual serialization formats that waste a lot of bandwidth and are expensive to parse and index. If RDF is meant to be machine understandable, why not use an appropriate format for that?

HDT (Header, Dictionary, Triples) is a **compact data structure** and **binary serialization format** for RDF that keeps big datasets **compressed** to save space while maintaining **search** and **browse** operations without prior decompression. This makes it an ideal format for storing and sharing RDF datasets on the Web."

JustGUI

Datafirst. 

Converting a CSV in RDF

Explorting RDF

Example Druid : Data Stories as Notebooks

## Question and Answers 



## Further Reading

