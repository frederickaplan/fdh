---
title: 'Foundations in Digital Humanities 2.11'
subtitle: 'Map Processing'
author:
 - Frederic Kaplan

# Don't change the following lines
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyfoot[L]{-release-version-}
output: pdf_document

---

# Map Processing

## Theses

1) Map processing focuses the automatic analysis of maps, with the aim of automatically or semiautomatically extracting geographic information of a pictorial or textual nature

2) Important progresses were made in the last years

## Map Databases

- USGS
- Gallica
- Oldmapsonline
- Harvard Map Collection
- ICGC
- David Rumsey Map Collection

Map corpora are potentially huge (even for a single place). Example :Paris Maps (Remi Petitpierre MS thesis, 1500+ maps). Despite the homogeneity of the subject, many challenges persist: differences in scales, codes, figurative grammars

Mapping informattion is not even distributed (cf Google Street View, OSM Maps)

## Map Preprocessing

Some challenges of digitised historical maps

- Digitization imperfactions : poor color, lost information due to binarization. law resolution
- Damaged maps or alternation due to conditioning

Super-resoultion :

- CNN-based super-resolution algorithm 

## Map realignment

Should realignment be done before segmentation or the contrary ?

### Georeferencing images using hand-picked homologous points

The georeferencing can be carried out using the georeferencing tool (e.g open source software QGIS) or Webased systems. 

For each images , between 5 and 7 points (sometime called Ground Control Points) can be chosen avoiding bridges, quais and road networks that might not be stable over time and favouring the corners of the main historical buildings. 

Affine transformation is sufficient. 

### GeoTIFF

The georeferenced images can be recorded in the GeoTIFF public domain format, allowing georeferencing information to be added to a TIFF image. 

## Map segmentation

(From Petitpierre 2020)

### Segmentation in classes

Example of 5 classes 

1) The frame i.e. the non-mapping part of the document. It contains the frame as well as the illustrations, illuminations, surrounding texts and medallions;

2) The road network. This class includes roads and streets as well as railroads and bridges;

3) Blocks: city blocks, detached houses, walls or anything similar to buildings;

4) Water: including rivers, canals, bodies of water, reservoirs, lakes, and sea;

5) Non-built which in fact includes all non-aquatic unbuilt land, including wasteland, meadows, crops, forests, but also urban unbuilt land, such as parks, enclosed squares, and inner courtyards.

Generic architecutre for segmentation (cf dh segment)

“The encoder reduces the spatial dimensions of the image, while increasing the number of layers, and thus the number of features that describe it, using operations that can be convolutions or poolings. In a way, the encoder allows the image to be abstracted to a higher level of understanding than what is possible with mere pixel values, in an effort of description. 

In a second step, a decoder performs the reverse path. It increases the size of the spatial features again, reduce the depth, and reconstructs an image: the output. The semantic output describes the type of objects recognized in the original image, in an interpretative effort.”

### Realignment based on segmentation

Realignment can then be done using for instance the road network and the classical methods using features points (e.g. SIFT or SURF) that were discussed in previous courses. 

### Automatic diachronic reordering 

The interest of chaining the plans lies in the reduction of calculation time. Indeed, the success of realigning a map is greater with maps that resemble each others. It is therefore more judicious to realign each map with the X maps that are closest to it than with all the maps each time. 

It is possible to calculate the morphological similarly between maps (see previous course). According to this approach, each map is linked to its closest neighboring X's within a directional graph. The relationships may therefore not be symmetrical.

## Cadastral computing

The cadaster is a database that links information with polygons.

- Segmentation and vectorization  : In 2017, a first study on the extraction of the cadastre showed promising results in terms of parcel extraction and recognition of identifiers. However, this method was entirely designed as an ad hoc pipeline adapted to a particular cadastre ( Oliveira et al. (2017)). The method used in this more generic study uses deep learning to segment the images of the cadastral map to identify plot borders and text segments (Oliveira et al. (2019)).  
  - The segmentation network is a fully convolutional neural network inspired by the U-Net architecture (Ronnenberg et al 2015). It uses a pre-formed network ResNet-50 (Ke et al 2016) as an encoder, which accelerates training, reduces the amount of training data required and aids generalisation. Details of this generic architecture are given in (Ares Oliveira et al 2018).  
  - Training : The neural network is trained to extract the contours and text of the plots using annotated data that correspond to about 1/3 of a map sheet from the 27 plates of the city of Venice. This annotation corresponds to about 800 plots.
- Transcription of identification numbers : The extracted borders are then used to calculate the parcel geometries to which the text segments are matched, which are transcribed using another artificial neural network. The identifier transcription network is a convolutional recurrent neural network that produces a string of characters when given an image segment containing text (Shi et al 2017). The network is trained on samples of identifiers from the Venetian archives and on numbers generated synthetically with the MNIST database (Lecun et al 1998).
- Performance : Intersection over Union. The performance of geometric extraction is measured by an IoU (Intersection over Union) calculation comparing the extracted geometries with 17000 annotated geometries. For a threshold value of IoU > 0.5, the recall level is 94% and the accuracy is 55%. This means that more than 9 out of 10 geometries could be extracted automatically. The average result in terms of accuracy mainly corresponds to more imprecise extractions on the road and canal network. 
- Performance of transcription : Concerning the transcription of plot numbers, the accuracy obtained is 92% and the recall 50%. All the detailed results are presented and discussed in Ares Oliveira et al (2019).
- GeoJSON export : The extraction of the vectorised parcels from the cadastral plan is saved as a GeoJSON file, an open format for simple geospatial data using the JSON (JavaScript Object Notation) standard.



## Implementation and Existing Systems

- Open Street Map. OSM Vector format can be edited with editors such as iD or JOSM
- MapBox : Good personalisation. Less data than Google but very interesitng anyway in particular for Pix2Pix application 
- QGIS
- Leaflet
- Tegola. Vector tiles make huge maps fast  Equivalent of image tiles for web mapping, applying the strengths of tiling — developed for caching, scaling and serving map imagery rapidly — to vector data. Image can be rendered client size (in the Browser)

## Diachronic map systems

### Places modelling

\- Names

\- Type (Natural geography, populated place, administrative unit i.e cadastral parcel)

\- Localisation in time (points, surface, volume)

### Gazeeteers and Geocoding services

Dictionary of places. 

- Google API. 
- Geonames, 
-  http://whgazetteer.org

Geocoding services

- Geopy : geopy is a Python client for several popular geocoding web services. geopy makes it easy for Python developers to locate the coordinates of addresses, cities, countries, and landmarks across the globe using third-party geocoders and other data sources. https://geopy.readthedocs.io/en/stable/

### Concrete Shape

Concrete shape : the actual and precise frontier of the place on the territory at a given moment in time

France is defined a series of diachronic concrete shapes. 

### Geometries

Each concrete shape can be approximate by different geometries that differ in terms of scale and ways of modelling (lines, polygons)

Is it better to approximate concrete forms as set of lines (one id per line) or as polygons (one id per polygons) ?

Better by lines. Coding geometries as set of lines gives more freedom and efficiency for coding evolution of concrete shapes. 

Lines can stored in a single table. 

### Diachronic evolution

Evolution of concrete shapes can be modelled at a very high precision (month or day). 

### Existing systems

- Running Reality
- Berreta work

## Summary

Progress in map processing permits to consider massive parallel treatment of historical maps and integration into standardised historical GIS.

## In the next chapter

We focus on 3D modelling

## Further reading

Book on Gazeeters

- Chiang, Y.-Y.,	 Leyk,	 S.,	 and	 Knoblock, C.	 A. (2014).  “A	 survey	 of	 digital	 map	 processing	 techniques,” ACM	 Computing	Surveys	(CSUR),	vol.	47,	no.	1,	p.	1,	2014
- Noizet, H.,	 Bove, B.,	 and	 Costa, L. (2013) “Paris	 de	 parcelles	en	pixels.”
- Gregory, I. N.,	 Kemp, K.	 K.,	 and	 Mostern,	 R. (2001).	 “Geographical	 information	 and	 historical	 research:	 Current	 progress	 and	 future	 directions,”	 History	 and	 Computing,	vol.	13,	no.	1,	pp.	7–23.

