---
title: 'Foundations in Digital Humanities 2.3'
subtitle: 'Writing Systems'
author:
 - Frederic Kaplan

# Don't change the following lines
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyfoot[L]{-release-version-}
output: pdf_document

---

# Writing Systems

## Theses

1) Many Writing Systems Exist

2) They get transformed when operationalised by a new technology (Printing press, Digital Computer)

3) Ways of writing handwritten and printed texts change over time. 

4) For creating digital representations of these texts one needs to combine powerful digital typography and universal textual representations standards

The course is organised in two parts. The first one is dealing with the evolution of Handwritten and Printed texts and the second one on the way to encode this evolution. 

## Scripts 

### Scripts and Writing Systems

Writing permits to record language in medium more permanent than speech, using a script. 

A script is a set of conventional symbols that can be used to represent in writing one or more languages. 

Components of a script are characters or more generally graphemes. 

A writing system is the use of a script in a particular language. The writing systems defines rules and convention on how to interpret the scripts. 

### Characters and Glyphs

The graphical representation of a character is called a Glyph. 

One character may be represented by different Glyphs (depending of positions (character sigma σ or [ς](https://en.wiktionary.org/wiki/ς#Greek) , arabic characters), handwritings, typefaces)

A Glyph may not correspond to a single character (& is a ligature of “e” and “t” and represents “et” or “and”)

### The Process of converting Speech into Text and Text into Speech

Scripts and Writing Systems formalisation can be interpreted as Speech2Text and Text2Speech technology. 

### Scripts Typologies

Many Scripts exist or have existed

Pictographic/ideographic : Ideograms representing Concepts (rather than Words) (Aztec, Adinkra used in modern Ghana, ..)

Logographic : Glyphs represent Words or Morphemes (Egyptian, Mayan, Chinese)

Syllabaries : Glyphs represent syllables (Japanese Kana, Cherokee)

Segmental scripts : Glyphs represent phonemes. Some have only consonants (Like Hebrew) and some are True Alphabet (Greek, Latin, Cyrillic)

Non linear alphabets are composed of something other than lines on a surface

\- Braille

\- Morse codes

\- International Maritime Signal Flags

### Undeciphered systems and documents 

There are still undeciphered systems that could be writing systems 

\- Cretan hieroglyphs

\- Indus Valley Civilization scripts

\- Linear A (Minoan)

\- Phaistos Disc

There are also recent manuscript writing in undeciphered and unidentified writing systems. Some of them could be hoaxes. 

E.g Voynich manuscript 

### Operationalisation of Scripts

Scripts get operationalised. Writing systems evolve over time. Most importanly, they can be operationalised using mechanic or digital technology. This causes profound transformation of their diffusion and usage. 

McLuhan : The Phonetic Alphabet and then the Printing Press have changed the human mind. With the creation of the movable type, the invention of the “Typographic man” at the Renaissance permits the standardisation and centralisation. For McLuhan, the electronics and the digital announces the a new era, a great mutation, a return to orality. 

Each writing system have a profound effect cognitive and cultural effect. They exert a “gravitational effect” on cognition.  Print technology made possible salient trends in the Western World : individualism, democracy, Protestantism, capitalism and nationalism. They are all based on "segmentation of actions and functions and principle of visual quantification”. 

## Handwritten texts

### Evolution of handwritten texts

Reading difficulty changes with time but paradoxically it is not necessary the older document that are the most difficult to recognised. 

The more writting skills get diffuse in the population, the less standardised are the handwritten documents. 

The handwriting found in some medieval manuscript is often more regular than modern handwriting. 

In the Carolingian minuscule, the script used in Charlemangne’s empire and its successor states (800-1200) letters and words are clearly separated. 

### Abbreviations

Medieval texts are highly “compressed”

One of the central skills in paleography is understanding abbreviations and expanding them correctly. 

Major reference works date from the 19th century.

\- Chassant (1845)

\- Trice Martin (1892)

\- Cappelli (1899)

Three systems of abbreviation in the antiquity (only indirect evidence as no Roman manuscript is known as an autograph) 

\- Tironian notae (system for making quick transcriptions)

\- Notae Iuris (abbreviations used by the legal profession)

\- Nomina Sacra (abbreviations found in early Christian writings)

Motivation for abbreviations : 

\- save space or to save time, but not only : 

\- language-independent communication in a multilingual environment 

\- the avoidance of using a sacred name 

\- allegoric, ritualistic, and occult purposes related to alchemical and magical symbols

Examples 

Suspension : abbreviating a word by omitting a number of characters at the end

AUG. ‘Augustus’ 

ib. ‘ibidem’ 

fq ‘filius quondam’

R.I.P. ‘Requiescat in pace’

D. ‘Dux’ or ‘Dominus’

MSS. ‘Manuscripta’ or ‘Manuscripts’

pp. ‘pages’

Contraction : omitting letters from the middle of the word.

caplo ‘capitulo’ 

ds ‘deus’

Signs which indicate the presence of an abbreviation

sign ***ꝯ\*** may stand for ‘con-’ and ‘com-’ when in word-initial position

The superscript letters: specific type of abbreviations in which part of the abbreviated word, often the last letter, is written above the line

w_t ‘with’

Special signs include symbols used for common words, monetary units (£ € $), alchemical, astrological, magical and hermetic symbols. 

### Handwritten text databases

MNIST and others

## Printed Texts

### The Printing Revolution 

Printed texts are noting but uniform in time. In practice they cover a 500y old period.

Printing with woodblocks started in China in the 9th century. 

Metal movable-type printing began to be used in Korea, 400y later. 

But it is in Europe that printing became a revolution. 

Before 1450, all books were written by hand. In 1455 the Gutenberg Bible was printed in Mainz. 

Fifty years after, millions of printed books circulated around Europe. Only 500 000 survive today. 

A technical-industrial process born in the middle of the 15th century in a strip of land between France and Germany, a kind of “Silicon Valley” of that time. 

The medieval world was changing with urbanisation and growth of mercantile classes. Demographic decline following the Black Death in 1348 and subsequent wars result in a a rise of labour costs and the search for innovation. 

The use of paper has driven down book-production costs.

Metal was taking over from wood in every technological process. 

And more importantly, there was a demand for more and more books. 

### Typefaces

Seven typefaces were developed im the 15th century. They imitate the hand of scribes. 

- Roman
- Gotic
- Batarde
- Greek
- Hebrew
- Glagolitico
- Italic

As most printers had their own sets cuts, there was a huge variety of letter-forms within a typeface style. 

### Redocumentaion of handwritten books

In the early years of printing the goal was to redocument as quickly and economically as possible handwritten books. The printed books needed to look as close to the manuscripts as possible. 

What survies today from 1450-1500: 28 0000 editions in 500 000 copies conserved in 4000 public libraries but also in un unknown number of private collections. 

The largest majority of them are in Latin (21 329), the language of communication across Europe. Latin is followed by German (3308) , Italien (2433) , French (1780), Deutsch (571), Spanish (437), English (240) and Hebrew (154).

(Figures from Dondi, Printing Revolution)

Most common subjects : Theology (4928), Law (4480), Literature and Classics (4313), Liturgy (2245), Devotional Literature (1893), Moral Literature (1499), Philosophy (1476), Current affaires (1041), Medicine (940), Ecclesiastical History (725), History (678), Bible (337), Natural Philosophy (265), Mathematics (98), Geographia (88), Agriculture and Music (55), Architecture and Engineering (22). 

(Figures from Dondi, Printing Revolution)

### Evolution of Typography

Letraset popularised Lorem Ipsum

The Macintosh introduced a confusion in terms of terminology.

A typeface is the design of lettering that can include variations, such as extra bold, bold, regular, light, italic, condensed, extended, etc. 

Each of these variations of the typeface is a font.

But many people use the term “Font” to refer to Typeface

## Spellings

Modern languages are taught in school, well-documented by grammars and dictionnaries. 

Modern languages have standard orthographies. This makes it possible to correct misspellings. 

Large amount of text are available for modern languages. Even for small languages, there are typically large newspaper or adminstrative corpora to train train text processing techniques. 

Historical languages often do not benefit from a standard orthography. A word may be spelled differently by the same author in the same text. Most Text Processing techniques assume that a word is always written in the same way. 

Spelling variation exist in modern language (e.g. American vs British spellings) but is much rather and well documented. 

Spelling is not limited to the spelling of individual words, it also concern punctuation, abbreviations, hyphenation, word separation. 

There are many examples of spelling change and rectification in French, Dutch, German, etc. 

This is also true for Chinese, Turkish, Korean, Mongolian. 

Change occurs diachronically but also synchronically. 

Often, most ancient text written in an historical form of the language get redocumented using the new form (sometimes more complex like in French)

Most digital historical texts are not originals but transcriptions. Different types of transcriptions exist

Diplomatic transcription : render every feature of the original text the can be reproduced including page layout, line breaks, original spelling and errors

At the other extreme :

Fully modernised transcription : Spelling and layout is transformed to facilitate reading and dissemination. Scholars may consider this a translation more than a transcription. 

Any transcription and redocumentation implies some interpretation and remapping. Therefore in most case the a digital text is the results of complex sequence of redocumentation process. 

In general, spelling variations do decrease over time. 

## Digital Encoding of Text

### Encoding Characters

#### ASCII

ASCII (American Standard Code for Information Interchange) was an early stage coded character set developed in 1964. 

It was fine for encoding basic texts in English but not adequate for most other language in Latin scripts and obvisouly not adapted for languages in other scripts. 

#### Unicode

Unicode is not concerned with Glyphs. Unicode is concerned with Character (abstract unit of meaning) and not in principle concerned with Glyphs (specific shape, and therefore typefaces and fonts). 

A font is "Unicode compliant" if the glyphs in the font can be accessed using code points defined in the Unicode standard.

Unicode considers diacritics (e.g. accents) as characters which have the property to combine graphically with the base character. 

Combining characters always follow the base character to which they apply. 

As a principle of economy, any new character that can be created as a combination of existing character will not be newly encoded. 

Unicode was progressively extent through the years and defines now code points for almost any scripts including runic, cuneiform, Linear B, Egyptian hieroglyphs. 

Still, some rare medieval characters are challenging and various initiatives are continuously working to deal with them and include them in Unicode. 

### Encoding Texts

#### XML

XML stands for eXtensibile Markup Language XML is a metalanguage for creating mark-up language. XML is a W3C recommandation. 

To write in XML you write text with tags : <atag> my text </atag>

This can be done in any text editor. 

1. XML is used to describe data, not to display them. XML does nothing. It describes.

2. XML tags are not predefined. You can define your own tags. This gives you a lot of freedom to describe the structure you want to describe.

3. When you are satisfied with your structure, you can fix our XML language by writing a DTD (Document Type Description). Thus, XML permits both fluidity and then rigor. 
4. XML is designed to be self-descriptive and “easily” readable. It is used to write “pivotal” descriptions in production chains

In the 50s, the first computers could not communicate with one another, if they were from different brands. 

In the 60s, IBM creates GML (Generalized Markup Language) to enable data exchanges and make the data structure explicit.

This is a great success. It becomes a standard : SGML (Standard Generalized Markup Language). The US fed gov. adopts it. 

In the 90s, Tim Berners-Lee at CERN creates the HTML language using a subset of SGML. 

HTML gets specialized in displaying data but does not impose a standard way for describing data. 

A group of researchers imagines another language to do this. The first version of XML is ready in 1998. XML is meant to be a database in plain text.

<BOOK>

 <FRONT>

  <TITLE>... </TITLE>

  <AUTHOR>...</AUTHOR>

 </FRONT>

 <BODY>

  <PART>

  <CHAPTER>...</CHAPTER>

  </PART>

 </BODY>

</BOOK>

A well-formed XML document follows the general rules of XML syntax.

A valid XML document follows the specific rules written in a DTD (Document Type Description)



<!DOCTYPE BOOK

[

<!ELEMENT BOOK (TITLE,AUTHOR,YEAR)>

<!ELEMENT TITLE (#PCDATA)>

<!ELEMENT AUTHOR (#PCDATA)>

<!ELEMENT YEAR (#PCDATA)>

]>

To use a DTD is not mandatory. But a DTD permits to agree on common XML dialect. 

Some software permit to check whether an XML file is valid compared to a given DTD.

The way an XML file is displayed can be specified in a CSS stylesheet.

A document can also be transformed using an XSLT script.

#### TEI

The Text Encoding Initiative (TEI) defines a a markup language for annoting texts with respects to their structural, visual and conceptual properties. 

The TEI Guidelines provide many examples for the representation of primary sources fo research and analysis.

A DTD is provided.

It is probably the largest XML schemata in existence.

… and it is fully open.

The TEI header contains the file description, a full bibliographic description of the computer file, an encoding description.

The actual content contained in a <text> element with paragraphs and lists, quotations, bibliographic references, etc. 

Specific modules are defined for particular text typologies (verse, drama, dictionaries), special documents (tables, formalulas, graphics)

TEI like any XML based system can be processed and displayed in numerous ways. 

TEI has the ambition of describing the content of entire document. But,as we have seen, document can have much more complex structure and circulation than XML trees. 

TEI has been developed in situation where a digital facsimilé is not available and act a standalone description of the document. 

In recent years, the increase of the number of documents available has changed slightly the situation. 

Part of burden for describing the document can now be done as part of a IIIF manifest. TEI remains a possible textual format for describing the textual content of some part of the document. 

Part of the TEI effort for formalising historical textual content can be reinterpreted in this new context using the logic of Regulated Representations 

#### Text as Graphs

(Tbd)

## Automatic Transcription

### OCR and HTR

OCR is said to be now state of the art.

Most OCR techniques use language specific dictionaries but also syntactic modelling.

We have seen that typefaces and spelling significatively change over time which makes OCR still a challenge for historical texts. 

This challenge was recognised early. For instance in 1974, Ray Kurzweil created an early Omni-font OCR technology. 

Deep-learning based Handwritten Text Recognition has made important progresses in recent years. The details of the methods will be discussed in the Algorithm parts. 

Pre and Post processing : OCR and HTR heavily rely on pre-processing techniques, to segment and prepare texts.

The use of crowdsourcing can be used to correct texts. 

This can also be done in an indirect way (e.g. reCAPTCHA)

Computer assisted transcription : New interfaces help transcribing by detecting lines and words and suggesting possible transcriptions. 

Big progress were made by the READ consortium and Transkribus interface. 

However, there are no “off-the-shelf” tool yet. 

### Handling Spelling Variations

(Most of this part from : Michael Piotrowski, Natural Language Processing for Historical Texts, Morgan and Claypool)

In digital historical texts spelling may be different because 

\- They differ from today’s orthography 

\- They are inconsistent in their use

\- They are been transcribed with errors. 

Canonical cognate : an equivalent of a historical word the preserves the morphological root and the morphosyntactic features of the historical form. 

Some historical words that have disappear may not have a canonical cognate. 

The redocumentation process that consists in mapping historical spellings to modern spelling is refered to as normalisation or canonicalisation. 

This process was introduced in the 19th century by German philologist Karl Lachmann. Critical editions were homogenising the variable spelling of original manuscripts to canonical forms. 

This is however different form mapping to a modern form of a the word. 

When indexing historical text in search engines, spelling modernisation is relevant. 

One possibly is to expand in the number of queries in real-time when a search is done, to include historical variants, or to perform approximate matching. 

Various methods 

\- Edit distance (Levensthtein) matching

\- Rule-based search

\- Grapheme-to-phoneme converters

\- Text-induced corpus clean-up

\- n-gram based methods

## Summary

Digital technology have made great process to represent digitally handwritten or printed texts in any language of works (past or present)

Automatic transcription is still a challenge but great progress are made. 

## In the next chapter 

We will see how to process large collection of texts

## Further reading

Dondi, Printing Revolution 

Richard Sproat, A Computational Theory of Writing Systems

Michael Piotrowski, Natural language processing for historical texts,  Morgan and Claypool

McLuhan, The Gutenberg Galaxy





